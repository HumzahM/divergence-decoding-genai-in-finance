import json

questions_in_each = {'mmlu_sociology': 201, 'mmlu_formal_logic': 126, 'mmlu_human_sexuality': 131, 'mmlu_econometrics': 114, 'mmlu_computer_security': 100, 'mmlu_college_physics': 102, 'mmlu_nutrition': 306, 'mmlu_global_facts': 100, 'mmlu_logical_fallacies': 163, 'mmlu_prehistory': 324, 'mmlu_professional_accounting': 282, 'mmlu_high_school_world_history': 237, 'mmlu_elementary_mathematics': 378, 'mmlu_anatomy': 135, 'mmlu_professional_psychology': 612, 'mmlu_human_aging': 223, 'mmlu_high_school_psychology': 545, 'mmlu_business_ethics': 100, 'mmlu_moral_scenarios': 895, 'mmlu_college_chemistry': 100, 'mmlu_high_school_mathematics': 270, 'mmlu_virology': 166, 'mmlu_moral_disputes': 346, 'mmlu_high_school_government_and_politics': 193, 'mmlu_marketing': 234, 'mmlu_high_school_microeconomics': 238, 'mmlu_professional_law': 1534, 'mmlu_jurisprudence': 108, 'mmlu_astronomy': 152, 'mmlu_security_studies': 245, 'mmlu_international_law': 121, 'mmlu_high_school_computer_science': 100, 'mmlu_conceptual_physics': 235, 'mmlu_clinical_knowledge': 265, 'mmlu_high_school_geography': 198, 'mmlu_high_school_chemistry': 203, 'mmlu_miscellaneous': 783, 'mmlu_medical_genetics': 100, 'mmlu_high_school_macroeconomics': 390, 'mmlu_high_school_us_history': 204, 'mmlu_college_medicine': 173, 'mmlu_electrical_engineering': 145, 'mmlu_high_school_physics': 151, 'mmlu_management': 103, 'mmlu_high_school_biology': 310, 'mmlu_college_biology': 144, 'mmlu_college_mathematics': 100, 'mmlu_us_foreign_policy': 100, 'mmlu_public_relations': 110, 'mmlu_professional_medicine': 272, 'mmlu_college_computer_science': 100, 'mmlu_machine_learning': 112, 'mmlu_high_school_european_history': 165, 'mmlu_high_school_statistics': 216, 'mmlu_abstract_algebra': 100, 'mmlu_world_religions': 171, 'mmlu_philosophy': 311}
questions_ran = {'mmlu_sociology': 5, 'mmlu_formal_logic': 2, 'mmlu_human_sexuality': 3, 'mmlu_econometrics': 2, 'mmlu_computer_security': 2, 'mmlu_college_physics': 2, 'mmlu_nutrition': 6, 'mmlu_global_facts': 2, 'mmlu_logical_fallacies': 3, 'mmlu_prehistory': 7, 'mmlu_professional_accounting': 5, 'mmlu_high_school_world_history': 5, 'mmlu_elementary_mathematics': 8, 'mmlu_anatomy': 2, 'mmlu_professional_psychology': 13, 'mmlu_human_aging': 4, 'mmlu_high_school_psychology': 11, 'mmlu_business_ethics': 2, 'mmlu_moral_scenarios': 18, 'mmlu_college_chemistry': 2, 'mmlu_high_school_mathematics': 5, 'mmlu_virology': 4, 'mmlu_moral_disputes': 7, 'mmlu_high_school_government_and_politics': 3, 'mmlu_marketing': 5, 'mmlu_high_school_microeconomics': 5, 'mmlu_professional_law': 31, 'mmlu_jurisprudence': 2, 'mmlu_astronomy': 3, 'mmlu_security_studies': 5, 'mmlu_international_law': 2, 'mmlu_high_school_computer_science': 2, 'mmlu_conceptual_physics': 5, 'mmlu_clinical_knowledge': 5, 'mmlu_high_school_geography': 4, 'mmlu_high_school_chemistry': 4, 'mmlu_miscellaneous': 16, 'mmlu_medical_genetics': 2, 'mmlu_high_school_macroeconomics': 8, 'mmlu_high_school_us_history': 4, 'mmlu_college_medicine': 3, 'mmlu_electrical_engineering': 3, 'mmlu_high_school_physics': 3, 'mmlu_management': 2, 'mmlu_high_school_biology': 6, 'mmlu_college_biology': 3, 'mmlu_college_mathematics': 2, 'mmlu_us_foreign_policy': 2, 'mmlu_public_relations': 2, 'mmlu_professional_medicine': 6, 'mmlu_college_computer_science': 2, 'mmlu_machine_learning': 2, 'mmlu_high_school_european_history': 3, 'mmlu_high_school_statistics': 5, 'mmlu_abstract_algebra': 2, 'mmlu_world_religions': 3, 'mmlu_philosophy': 6}

target_file = "model_eval/topk250_ma/mmlu_cot_0shot/results_2025-08-30T16-26-37.192445.json"

with open(target_file) as f:
    data = json.load(f)
    data = data["results"]

number_correct_strict = 0

for key in questions_in_each.keys():
    number_correct_strict += int(data[key]["exact_match,strict-match"] * questions_in_each[key]) #Floating point issues but everything is at worst a 0.99999 or 0.00001

print(f"Strict Match: {number_correct_strict}/{sum(questions_ran.values())} = {number_correct_strict/sum(questions_ran.values()):.4f}")